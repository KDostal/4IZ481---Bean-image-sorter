{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cde6fbd",
   "metadata": {},
   "source": [
    "## Vizu√°ln√≠ predikce chorob u fazol√≠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036e4ae",
   "metadata": {},
   "source": [
    "Nejd≈ô√≠ve importujeme knihovny a dataset z HuggingFace Api a zobraz√≠me nƒõjak√© p≈ô√≠klady.\n",
    "Pot√© u≈æ se pust√≠me do tvorby a dolaƒèƒõn√≠ modelu, postup je intuitivn√≠. V procesu pou≈æ√≠v√°me knihovny transformers a PyTorch \n",
    "(knihovna transformers se bez PyTorch neobejde),\n",
    "kdy t≈ô√≠dƒõ Trainer poskytneme data plus nutn√© parametry a t√≠m vytvo≈ô√≠me fin√°ln√≠ model k natr√©nov√°n√≠ na na≈°ich datech,\n",
    "\n",
    "Trainer: https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "\n",
    "Dataset: https://huggingface.co/datasets/beans\n",
    "\n",
    "Inspirace: https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d18649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import knihoven, nap≈ôed mus√≠me m√≠t nainstalovan√© v≈°echny knihovny, nap≈ô:\n",
    "# pip3 install torch torchvision torchaudio\n",
    "# pip install datasets transformers\n",
    "# conda install -c conda-forge tensorboard\n",
    "# conda install -c conda-forge protobuf\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import Trainer, ViTFeatureExtractor, ViTForImageClassification, TrainingArguments\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e866b",
   "metadata": {},
   "source": [
    "### Naƒçten√≠ datasetu a modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0697bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset beans (C:/Users/Gigon/.cache/huggingface/datasets/beans/default/0.0.0/90c755fb6db1c0ccdad02e897a37969dbf070bed3755d4391e269ff70642d791)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3634501715be4456a43a432d21b2b04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image_file_path', 'image', 'labels'],\n",
       "        num_rows: 1034\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image_file_path', 'image', 'labels'],\n",
       "        num_rows: 133\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image_file_path', 'image', 'labels'],\n",
       "        num_rows: 128\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naƒçten√≠ datov√© sady d√≠ky huggingface knihovnƒõ datasets viz. https://huggingface.co/docs/datasets/index\n",
    "ds = load_dataset('beans')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf98933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_file_path': 'C:\\\\Users\\\\Gigon\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\ab87c331001da2f7769bdbbf7c3596bbfb41d2845c97674c7b502aab7f668023\\\\train\\\\angular_leaf_spot\\\\angular_leaf_spot_train.136.jpg',\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500>,\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zobrazen√≠ obr√°zku\n",
    "image = ds['train'][42]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847b7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naƒçten√≠ modelu z Huggingface a vytvo≈ôen√≠ feature extractoru (ten z obr√°zku vr√°t√≠ hodnoty pixel≈Ø) podle n√°vodu https://huggingface.co/docs/transformers/main_classes/extractor\n",
    "huggingface_model = 'google/vit-base-patch16-224-in21k' # zde jsem narazil chybovou hl√°≈°ku, ≈æe m≈Øj PC nepodporuje symlinks a musel jsem povolit Windows Developer mode viz. https://learn.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
    "extractor = ViTFeatureExtractor.from_pretrained(huggingface_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c84454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.0510, -0.0510, -0.1059,  ..., -0.0196,  0.0510,  0.0039],\n",
       "          [-0.0510, -0.0902, -0.0902,  ...,  0.1137, -0.0275, -0.1529],\n",
       "          [-0.0980, -0.2000, -0.0588,  ...,  0.1765, -0.0745, -0.1608],\n",
       "          ...,\n",
       "          [ 0.0980,  0.0196,  0.0588,  ..., -0.3490, -0.5216, -0.6314],\n",
       "          [-0.0902, -0.0980, -0.0431,  ..., -0.4039, -0.5294, -0.4275],\n",
       "          [-0.2000, -0.1686, -0.0980,  ..., -0.4980, -0.5216, -0.3647]],\n",
       "\n",
       "         [[-0.3255, -0.3098, -0.3490,  ..., -0.1608, -0.0745, -0.1216],\n",
       "          [-0.3255, -0.3490, -0.3333,  ..., -0.0431, -0.1765, -0.3020],\n",
       "          [-0.3647, -0.4588, -0.3176,  ...,  0.0039, -0.2549, -0.3333],\n",
       "          ...,\n",
       "          [ 0.4431,  0.3804,  0.4353,  ..., -0.3176, -0.5294, -0.6706],\n",
       "          [ 0.3412,  0.3333,  0.3882,  ..., -0.3725, -0.5294, -0.4275],\n",
       "          [ 0.2784,  0.3020,  0.3569,  ..., -0.4667, -0.5059, -0.3569]],\n",
       "\n",
       "         [[-0.5608, -0.5294, -0.5686,  ..., -0.3725, -0.3569, -0.4196],\n",
       "          [-0.5529, -0.5686, -0.5451,  ..., -0.2941, -0.4353, -0.5529],\n",
       "          [-0.5922, -0.6706, -0.5137,  ..., -0.3020, -0.4824, -0.5373],\n",
       "          ...,\n",
       "          [-0.4353, -0.5294, -0.4588,  ..., -0.8510, -0.9373, -0.9922],\n",
       "          [-0.6627, -0.6863, -0.6314,  ..., -0.8667, -0.9686, -0.9686],\n",
       "          [-0.7882, -0.7725, -0.7255,  ..., -0.9216, -0.9608, -0.9451]]]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor(image[\"image\"], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f930dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c23c1",
   "metadata": {},
   "source": [
    "### P≈ô√≠prava nutn√Ωch funkc√≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea46069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkce pro p≈ôeveden√≠ datasetu obr√°zk≈Ø na dataset hodnot pixel≈Ø\n",
    "def get_pixels(images):\n",
    "    inputs = extractor([x for x in images['image']], return_tensors='pt')\n",
    "\n",
    "    # plus p≈ôidat labels\n",
    "    inputs['labels'] = images['labels']\n",
    "    return inputs\n",
    "\n",
    "# dal≈°√≠ povinn√° funkce pro Trainer\n",
    "def get_metrics(p):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    \n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
    "\n",
    "arguments = TrainingArguments( # definice specifick√Ωch parametr≈Ø pro dotr√©nov√°n√≠ modelu\n",
    "  output_dir=\"./vit-base-beans\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# funkce nutn√° pro fungov√°n√≠ Traineru, rozbaluje vstupn√≠ batche do v√Ωstupn√≠ viz. https://huggingface.co/docs/transformers/main_classes/data_collator\n",
    "def collator(batch): \n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb1a5b3",
   "metadata": {},
   "source": [
    "### Tr√©nov√°n√≠ a vyhodnocen√≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd4028a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\Soubory\\Anaconda\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1034\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 260\n",
      "  Number of trainable parameters = 85800963\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [260/260 49:46, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.094700</td>\n",
       "      <td>0.032640</td>\n",
       "      <td>0.992481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.033406</td>\n",
       "      <td>0.992481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 133\n",
      "  Batch size = 8\n",
      "D:\\Soubory\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "Saving model checkpoint to ./vit-base-beans\\checkpoint-100\n",
      "Configuration saved in ./vit-base-beans\\checkpoint-100\\config.json\n",
      "Model weights saved in ./vit-base-beans\\checkpoint-100\\pytorch_model.bin\n",
      "Image processor saved in ./vit-base-beans\\checkpoint-100\\preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 133\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans\\checkpoint-200\n",
      "Configuration saved in ./vit-base-beans\\checkpoint-200\\config.json\n",
      "Model weights saved in ./vit-base-beans\\checkpoint-200\\pytorch_model.bin\n",
      "Image processor saved in ./vit-base-beans\\checkpoint-200\\preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./vit-base-beans\\checkpoint-100 (score: 0.03264022246003151).\n",
      "Saving model checkpoint to ./vit-base-beans\n",
      "Configuration saved in ./vit-base-beans\\config.json\n",
      "Model weights saved in ./vit-base-beans\\pytorch_model.bin\n",
      "Image processor saved in ./vit-base-beans\\preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         4.0\n",
      "  total_flos               = 298497957GF\n",
      "  train_loss               =      0.1154\n",
      "  train_runtime            =  0:50:16.01\n",
      "  train_samples_per_second =       1.371\n",
      "  train_steps_per_second   =       0.086\n"
     ]
    }
   ],
   "source": [
    "ds_pixels = ds.with_transform(get_pixels) # aplikace funkce get_pixels na cel√Ω dataset\n",
    "\n",
    "final_model = ViTForImageClassification.from_pretrained(\n",
    "    huggingface_model,\n",
    "    num_labels=len(ds['train'].features['labels'].names),\n",
    "    id2label={str(i): c for i, c in enumerate(ds['train'].features['labels'].names)},\n",
    "    label2id={c: str(i) for i, c in enumerate(ds['train'].features['labels'].names)}\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=arguments,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=get_metrics,\n",
    "    train_dataset=ds_pixels[\"train\"],\n",
    "    eval_dataset=ds_pixels[\"validation\"],\n",
    "    tokenizer=extractor,\n",
    ")\n",
    "\n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb71461e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 133\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 128\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.9925\n",
      "  eval_loss               =     0.0326\n",
      "  eval_runtime            = 0:00:32.89\n",
      "  eval_samples_per_second =      4.043\n",
      "  eval_steps_per_second   =      0.517\n",
      "***** test metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.9453\n",
      "  eval_loss               =     0.1801\n",
      "  eval_runtime            = 0:00:36.92\n",
      "  eval_samples_per_second =      3.467\n",
      "  eval_steps_per_second   =      0.433\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(ds_pixels['validation'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "metrics = trainer.evaluate(ds_pixels['test'])\n",
    "trainer.log_metrics(\"test\", metrics)\n",
    "trainer.save_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57d7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
